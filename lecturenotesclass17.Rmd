---
title: "Linear Regression I"
author: ""
date: "March 22, 2021"
output: html_document
editor_options: 
  chunk_output_type: console
---

## Coming up
- Project proposal due today at 11:59 PM
- Homework 4 due Thursday at 11:59 PM
- Exam 2 next week.

## Main ideas

- Learn about what we use models for.
- Begin to learn the language of linear models in R.

## Modeling

- We use models to...
  - understand relationships
  - assess differences
  - make predictions

- We will focus on **linear** models but there are many other types.

# Packages

In addition to using the `tidyverse` and `viridis` packages, today we will be using data from the `fivethirtyeight` package with data on candy rankings from this 2017 [article](https://fivethirtyeight.com/videos/the-ultimate-halloween-candy-power-ranking/). The variables in this dataset are discussed in more detail [here](https://github.com/fivethirtyeight/data/tree/master/candy-power-ranking). We'll also be using the `broom` package, which  takes the messy output of built-in functions in R, such as `lm`, and turns them into tidy data frames.

```{r packages, include=FALSE}
library(tidyverse)
library(fivethirtyeight)
library(broom)
library(viridis)
```

# Distribution of Variables

Let's start by looking at the variables in this dataset.
```{r glimpse}
glimpse(candy_rankings)
```

Three numerical variables of potential interest in this dataset are the sugar percentile, the unit price percentile, and the percentage of time that the candy bar won against its competitors. Let's look at the distribution of these variables. What units are these variables measured in?

```{r sugarhist}
ggplot(data = candy_rankings, aes(x = sugarpercent)) +
  geom_histogram(binwidth = .1) +
  labs(title = "Distribution of Sugar Percentile", x = "Sugar Percentile", y =
         "Count")
```

```{r pricehist}
ggplot(data = candy_rankings, aes(x = pricepercent)) +
  geom_histogram(binwidth = .1) +
  labs(title = "Distribution of Price Percentile", x = "Price Percentile", y =
         "Count")
```

```{r winhist}
ggplot(data = candy_rankings, aes(x = winpercent)) +
  geom_histogram(binwidth = 10) +
  labs(title = "Distribution of Win Percent", x = "Win Percentile", y = "Count")
```

Let's say you wanted to see if sugary candies are more likely to win. First, we notice that the sugar and price percentiles are on a scale from 0 to 1, while the win percentage variable is on a scale from 0 to 100. Let's change the sugar and price variables to the same scale as win percentage by creating new variables on a 0 to 100 scale to be on the same scale as win percentage. 

```{r mutatesugar}
candy_rankings<- candy_rankings %>%
  mutate(sugarpercent100 = sugarpercent * 100)
```

```{r mutateprice}
candy_rankings<- candy_rankings %>%
  mutate(pricepercent100 = pricepercent * 100)
```

```{r visualizingmodel}
ggplot(data = candy_rankings, aes(x = sugarpercent100, y = winpercent)) + 
  labs(title= "Do Sugary Candies Win More Often?", x = "Sugar Percentile", y =  
       "Win Percentage") + 
  geom_point() +
  geom_smooth(method = "lm")
```

## Some Terminology

- Response Variable: Variable whose behavior or variation you are trying to understand, on the y-axis. Also called the dependent variable.

- Explanatory Variable: Other variables that you want to use to explain the variation in the response, on the x-axis. Also called independent variables, predictors, or features.

- Predicted value: Output of the model function
  - The model function gives the typical value of the response variable   conditioning on the explanatory variables (what does this mean?)

- Residuals: Shows how far each case is from its predicted value
  - Residual = Observed value - Predicted value
  - Tells how far above/below the model function each case is

**Question**: What does a negative residual mean? Which candies on the plot have have negative residuals, those below or above the line?


```{r resids}
ggplot(data = candy_rankings, aes(x = sugarpercent100, y = winpercent)) + 
  labs(title = "Do Sugary Candies Win More Often?", x = "Sugar Percentile", y = 
         "Win Percentage") + 
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, col = "blue", lty = 1, lwd = 1) + 
  geom_segment(aes(x = 73.2, y = 66.9, xend = 73.2, yend = 53.38), col = "red") +
  geom_point(aes(x = 73.2, y = 66.9), col = "red", shape = 1, size = 4) +
   annotate("text", x = 73.2, y = 70, label = "66.9 - 53.4 = 13.5", color =
              "red", size = 4)
```

## Multiple Explanatory Variables

How, if at all, does the relationship between sugar percentile and win percentage vary by whether or not the candy is chocolate?

```{r multiplevars}
ggplot(data = candy_rankings, aes(x = sugarpercent100, y = winpercent,
                                  color = factor(chocolate))) + 
  scale_color_viridis(discrete = TRUE, option = "D", name = "Chocolate?") + 
  labs(title = "Do Sugary Candies Win More Often? The Role of Chocolate", x =
         "Sugar Percentile", y = "Win Percentage") +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) 
```

We will talk about multiple explanatory variables in the following weeks.

## Models - upsides and downsides

- Models can sometimes reveal patterns that are not evident in a graph of the
data. This is a great advantage of modeling over simple visual inspection of
data. 

- There is a real risk, however, that a model is imposing structure that is
not really there on the scatter of data, just as people imagine animal shapes in the stars. A skeptical approach is always warranted.

## Variation in the Model

- This is just as important as the model, if not more!

- Statistics is the explanation of variation in the context of what remains unexplained.

- The scatter suggests that there might be other factors that account for large parts of candy win percentage variability, or perhaps just that randomness plays a big role.

- Adding more explanatory variables to a model can sometimes usefully reduce the size of the scatter around the model. (We'll talk more about this later.)

## How do we use models?

1. Explanation: Characterize the relationship between *y* and *x* via slopes for numerical explanatory variables or differences for categorical explanatory variables.

2. Prediction: Plug in *x*, get the predicted *y*.

## Intepreting Models

Here, we will be using the `broom` package to clean up our regression output:

- **`broom`** follows tidyverse principles and tidies up regression output

- **`tidy`**: Constructs a tidy data frame summarizing model's statistical findings

- **`glance`**: Constructs a concise one-row summary of the model

- **`augment`**: Adds columns (e.g. predictions, residuals) to the original data that was modeled

Returning to our earlier question, do sugary candies win more often?

```{r sugarmodel}
sugarwins <- lm(winpercent ~ sugarpercent100, data = candy_rankings)
tidy(sugarwins)
```

$$\widehat{WinPercent} = 44.6 + 0.12~SugarPercentile$$

- Slope: For each additional percentile of sugar, the win percentage is expected to be higher, on average, by 0.12 percentage points.

- **Question**: Why is it important to know what units the variable is measured in here? Why did we change the scale of percentile here earlier? 
  
- Intercept: Candies that are in the 0th percentile for sugar content are expected to win 44.6 percent of the time, on average.

    - Does this make sense?
    
## The linear model with a single predictor

- We're interested in the $\beta_0$ (population parameter for the intercept)
and the $\beta_1$ (population parameter for the slope) in the 
following model:

$$ {y} = \beta_0 + \beta_1~x + \epsilon $$

- Unfortunately, we can't get these values


- So we use sample statistics to estimate them:

$$ \hat{y} = b_0 + b_1~x $$


## Least squares regression

The regression line minimizes the sum of squared residuals.

- **Residuals**: </font> $e_i = y_i - \hat{y}_i$,

- The regression line minimizes $\sum_{i = 1}^n e_i^2$.

- Equivalently, minimizing $\sum_{i = 1}^n [y_i - (b_0 + b_1~x_i)]^2$

- **Question**: Why do we minimize the *squares* of the residuals?

## Visualizing Residuals

```{r vizresids}

sugarwins <- lm(winpercent ~ sugarpercent100, data = candy_rankings)

sugarwins %>% 
  augment() %>% 
  ggplot(mapping = aes(x = sugarpercent100, y = winpercent)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = FALSE) +
  geom_point(mapping = aes(y = .fitted)) +
  geom_segment(mapping = aes(xend = sugarpercent100, yend = .fitted), 
               alpha = 0.4, color="red") +
  ylim(0, 100) +
  labs(x = "Sugar percentage", y = "Win percentage") +
  theme_bw()
```

Check out the applet [here](http://www.rossmanchance.com/applets/RegShuffle.htm) to see this process in action.

## Properties of the least squares regression line

- The estimate for the slope, $b_1$, has the same sign as the correlation between the two variables.

- The regression line goes through the center of mass point, the coordinates corresponding to average $x$ and average $y$: $(\bar{x}, \bar{y})$

- The sum of the residuals is zero: 

$$\sum_{i = 1}^n e_i = 0$$

- The residuals and $x$ values are uncorrelated.

## Categorical Predictors

- Non-continuous predictors: Let's say that we want to see if candies with chocolate are more popular than those without chocolate. Our chocolate variable has two categories: chocolate and not chocolate. Predictors such as this are known as **dummy variables** and are typically coded as "1" (if true) and "0" (if false).

Let's run a model with the chocolate variable.

```{r chocolate}
chocolatewins <- lm(winpercent ~ factor(chocolate), data = candy_rankings)
tidy(chocolatewins)
```

- **Slope:** Candies with chocolate are expected, on average,
to have a winning percentage 18.8 percentage points higher than candies that don't have chocolate.
    - Compares baseline level (`chocolate = 0`) to other level
    (`chocolate = 1`).

- **Intercept:** Candies that don't have chocolate are expected, on average, to win 42.1 percent of the time. 

## What about categorical predictors with more than two levels.

Let's say that you are interested in which combination of chocolate and peanuts/almonds is the most popular. What would you do to create a variable to represent each of these options. Let's use `case_when` here.

```{r createnewvar}
candy_rankings<-candy_rankings %>%
   mutate(choc_peanutyalmondy = 
            case_when(chocolate == 1 & peanutyalmondy == 0 ~ 'just chocolate',
                      chocolate == 1 & peanutyalmondy == 1 ~ 'both', 
                      chocolate == 0 & peanutyalmondy == 1 ~
                        'justpeanutyalmondy',
                      chocolate == 0 & peanutyalmondy == 0 ~ "neither"))
```

Now, let's run the model.

```{r bestcombo}
bestcombowins <- lm(winpercent ~ factor(choc_peanutyalmondy), data = candy_rankings)
tidy(bestcombowins)
```

When the categorical explanatory variable has many levels, the levels are encoded to dummy variables.

Each coefficient describes the expected difference between combinations of chocolate in that particular candy compared to the baseline level.

**Question**: Notice here that we have three variables in the model. Why don't we have four since there are four possibilities? Why don't we just use one variable here by adding the two categories together?

## Correlation does not imply causation!

Remember this when interpreting model coefficients.

# Prediction with Models

## Do Candies with more ingridents do better?

Does the number of ingredients in a candy bar make it do better? Let's create a variable measuring the number of ingredients and see how well it does.

```{r ingredientsvariable}
candy_rankings <- candy_rankings %>%
  mutate(number_ingredients = chocolate + fruity + caramel + peanutyalmondy +
           nougat + crispedricewafer)
```

Do you think a candy bar with all of these things at the same time sounds good?

```{r ingredientsmodel}
ingredientsmodel <- lm(winpercent ~ number_ingredients, data = candy_rankings)
tidy(ingredientsmodel)
```

On average, what percentage of the time would you expect a candy bar with four ingredients to win?

$$\widehat{WinPercentage} = 35.5 + 10.7~Ingredients_{number}$$
```{r prediction}
35.5 + 10.7 * 4
```

"On average, we expect candies with four ingredients to win 78.3 percent of the time."

Warning: We "expect" this to happen, but there will be some variability. (We'll learn about measuring the variability around the prediction later.)

On average, how often would you expect a candy with all seven ingredients to win? Is this realistic? Do any bars like this actually exist in the data?

```{r calcwinall}

```

Would you eat a candy bar with all seven ingredients?

## Watch out for extrapolation!

"When those blizzards hit the East Coast this winter, it proved to my satisfaction that global warming was a fraud. That snow was freezing cold. But in an alarming trend, temperatures this spring have risen. Consider this: On February 6th it was 10 degrees. Today it hit almost 80. At this rate, by August it will be 220 degrees. So clearly folks the climate debate rages on."
-Stephen Colbert, April 6th, 2010 (OIS, page 322)

## Tidy regression output

Let's revisit the model predicting win percentage from sugarcontent:

```{r revistmodel}
sugarwins <- lm(winpercent ~ sugarpercent100, data = candy_rankings)
```

## Not-so-tidy regression output

- You might come across these in your googling adventures, but we'll try to stay away from them

- Not because they are wrong, but because they don't result in tidy data frames as results.

## Not-so-tidy regression output (1)

Option 1:

```{r option1}
sugarwins
```

## Not-so-tidy regression output (2)

Option 2:

```{r option2}
summary(sugarwins)
```

What makes a data frame tidy?

1. Each variable must have its own column.
2. Each observation must have its own row.
3. Each value must have its own cell.

## Tidy regression output

Achieved with functions from the broom package:

- **`tidy`**: Constructs a data frame that summarizes the model's statistical findings. We've talked about coefficient estimates and standard errors, but it also displays *test statistics and p-values* (more on these in a few weeks!).

- **`augment`**: Adds columns to the original data that was modeled. This includes predictions and residuals.

- **`glance`**: Constructs a concise one-row summary of the model, computed once for the entire model. 

## Tidy your model's statistical findings

```{r tidy1}
tidy(sugarwins)
```


```{r tidy2}
tidy(sugarwins) %>%
  select(term, estimate) %>%
  mutate(estimate = round(estimate, 3))
```

## Practice

1. Do you get your money's worth when buying expensive candy? First, use `ggplot` to visualize the relationship between cost percentile and win percentage as we did for sugar percentile and win percentage.

```{r costviz}

```

Then, run and interpret a linear model with win percentage as your response variable and cost percentile as your dependent variable.

```{r costmodel}

```

**Interpretation**: 

2. How popular are chocolates that have caramel? Run a linear model with caramel as the explanatory variable and win percentage as the response variable and interpret it.

```{r caramodel}

```

**Interpretation**:

3. Which candy is the sweetest: chocolate, caramel, candies with both, or candies with neither? Create a variable that represents this use `case_when` and then run and interpret a linear model with sugar content as the dependent variable.

```{r sweetestcandy}
```

```{r runmodel}

```

## Additional Resources
- [https://r4ds.had.co.nz/model-basics.html](R for Data Science)

## Tasks For Next Class
- Please read OIS section 8.4 on inference for linear regression.